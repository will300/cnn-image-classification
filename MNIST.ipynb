{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network for MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the relevant modules are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the data for training and testing is loaded and stored in the x and y variables for images and labels. The data comes from a dataset called MNIST which contains images of handwritten digits from 0-9. The loading process can be done using a tensorflow command once the MNIST dataset has been downloaded to a PC: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 60000\n",
      "Number of testing images: 10000\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
    "print(\"Number of training images: {}\".format(len(x_train)))\n",
    "print(\"Number of testing images: {}\".format(len(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are loaded in the form of 28 x 28 arrays with values of 0-256, which can be displayed as pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Class: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[22], cmap='gray')\n",
    "print(\"Image Class: {}\".format(y_train[22]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label for each image is in the form of an integer and needs to be 'one hot encoded', where it is turned into an array of nine 0s and one 1 with the position of the 1 within the array denoting the label. For instance 9 would become [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 1 ]. This is because the integers are really a categorical form of data, so it is more appropriate to predict the output in this way rather than a single floating point number between 0 and 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/yes/envs/py3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "\n",
    "y_train_one_hot = one_hot_encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow placeholder tensors are created, which are empty tensors with dimensions ready to receive tensors. One receives batches of input images, one their corresponding labels, and the other the learning rate of the CNN. This is the most useful way of including changeable tensors in a Tensorflow program. The None dimension is included so that different sizes of image batches can be fed into the network in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs():\n",
    "\n",
    "    input_tensor = tf.placeholder(tf.float32, (None, 28, 28, 1))\n",
    "    output_labels = tf.placeholder(tf.float32, (None, 10))\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    \n",
    "    return (input_tensor, output_labels, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions are written separately to build the structure of the network, calculate the loss function at the output, and define the optimizer algorithm. This architecture was chosen arbitrarily and will need refining for a more complex data set, although it does use a general rule of decreasing the height and width of layers from input to output while increasing the depth. The filters in each layer have dimensions 5 x 5, and they are moved over the previous layer with a stride length of 2. The number of filters increases from 32 in the first layer to 64 in the next and then 128. Relu activation functions and batch normalization are used on the outputs of individual layers to allow the network to generalize to different data better and improve training. The last convolutional layer is flattened and connected to a dense fully connected layer with 10 output nodes for the 10 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(input_tensor, reuse=None):\n",
    "    \n",
    "    with tf.variable_scope('network', reuse=reuse):\n",
    "    \n",
    "        # Tensor size: 28x28\n",
    "        input_layer = tf.layers.conv2d(input_tensor, 32, 5, 2, padding='same')\n",
    "        input_layer = tf.nn.relu(input_layer)\n",
    "    \n",
    "        # Tensor size: 14x14\n",
    "        hidden_layer_1 = tf.layers.conv2d(input_layer, 64, 5, 2, padding='same')\n",
    "        hidden_layer_1 = tf.layers.batch_normalization(hidden_layer_1, training=True)\n",
    "        hidden_layer_1 = tf.nn.relu(hidden_layer_1)\n",
    "    \n",
    "        # Tensor size: 7x7\n",
    "        hidden_layer_2 = tf.layers.conv2d(hidden_layer_1, 128, 5, 2, padding='same')\n",
    "        hidden_layer_2 = tf.layers.batch_normalization(hidden_layer_2, training=True)\n",
    "        hidden_layer_2 = tf.nn.relu(hidden_layer_2)\n",
    "    \n",
    "        # Tensor size: 4x4\n",
    "        flat_layer = tf.reshape(hidden_layer_2, (-1, 4*4*128))\n",
    "    \n",
    "        output_layer = tf.layers.dense(flat_layer, 10)\n",
    "        logits = output_layer\n",
    "        output = tf.sigmoid(logits)\n",
    "        \n",
    "    return logits, output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function uses cross entropy loss after taking sigmoid values of the logits to return them as probabilities between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, labels):\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer function uses an Adam algorithm for backpropagation. The optimizer is preceded by a control dependencies command in order to allow the batch normalization process to update population statistics on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate):\n",
    "    \n",
    "    train_vars = tf.trainable_variables()\n",
    "    params = [var for var in train_vars if var.name.startswith('network')]\n",
    "    \n",
    "    ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    upd = [op for op in ops if op.name.startswith('network')]\n",
    "    \n",
    "    with tf.control_dependencies(upd):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss=loss, var_list=params)\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train function calls the each of the previous functions. During each batch iteration, batches of images and labels are fed into the respective placeholder tensors inside the sess.run method. At the end of the training process all of the trained weights within the network are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_features, input_labels, epoch_count, batch_size, learn_rate):\n",
    "    \n",
    "    input_tensor, output_labels, learning_rate = build_inputs()\n",
    "    logits, output = build_cnn(input_tensor)\n",
    "    loss = build_loss(logits, output_labels)\n",
    "    opt = build_optimizer(loss, learn_rate)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        steps = 0\n",
    "        \n",
    "        for current_epoch in range(epoch_count):\n",
    "            \n",
    "            for current_batch in range(int(len(input_features)/batch_size)):\n",
    "                \n",
    "                batch_images = input_features[current_batch * batch_size : current_batch * batch_size + batch_size, :, :]\n",
    "                batch_images = np.reshape(batch_images, (batch_size, 28, 28, 1))\n",
    "                batch_labels = input_labels[current_batch * batch_size : current_batch * batch_size + batch_size, :]\n",
    "                \n",
    "                steps += 1\n",
    "                \n",
    "                _ = sess.run(opt, feed_dict={input_tensor: batch_images, output_labels: batch_labels, learning_rate: learn_rate})\n",
    "            \n",
    "                if steps % 100 == 0:\n",
    "                    current_loss = loss.eval({input_tensor: batch_images, output_labels: batch_labels})\n",
    "                    print(\"Current epoch: {}/{}\".format(current_epoch + 1, epoch_count))\n",
    "                    print(\"Current loss: {}\".format(current_loss))\n",
    "        \n",
    "        saver.save(sess, \"/tmp/model.ckpt\")           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch: 1/1\n",
      "Current loss: 0.08731301873922348\n",
      "Current epoch: 1/1\n",
      "Current loss: 0.04341135546565056\n",
      "Current epoch: 1/1\n",
      "Current loss: 0.06348304450511932\n",
      "Current epoch: 1/1\n",
      "Current loss: 0.061525214463472366\n",
      "Current epoch: 1/1\n",
      "Current loss: 0.04595217853784561\n",
      "Current epoch: 1/1\n",
      "Current loss: 0.030772468075156212\n",
      "Current epoch: 1/1\n",
      "Current loss: 0.013633126392960548\n",
      "Current epoch: 1/1\n",
      "Current loss: 0.017124151811003685\n",
      "Current epoch: 1/1\n",
      "Current loss: 0.04435595124959946\n",
      "Current epoch: 1/1\n",
      "Current loss: 0.016151878982782364\n",
      "Current epoch: 1/1\n",
      "Current loss: 0.013212062418460846\n",
      "Current epoch: 1/1\n",
      "Current loss: 0.047010842710733414\n",
      "Current epoch: 1/1\n",
      "Current loss: 0.014372485689818859\n",
      "Current epoch: 1/1\n",
      "Current loss: 0.007781213615089655\n",
      "Current epoch: 1/1\n",
      "Current loss: 0.006052855867892504\n",
      "Current epoch: 1/1\n",
      "Current loss: 0.01439860463142395\n",
      "Current epoch: 1/1\n",
      "Current loss: 0.012702465057373047\n",
      "Current epoch: 1/1\n",
      "Current loss: 0.006778703071177006\n"
     ]
    }
   ],
   "source": [
    "epoch_count = 1\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():\n",
    "    train(x_train, y_train_one_hot, epoch_count, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function loads the trained weights from file, feeds a test image into the network and classifies it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(input_data):\n",
    "    \n",
    "    input_tensor, output_labels, learning_rate = build_inputs()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        \n",
    "        saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "        \n",
    "        logits, output = build_cnn(input_tensor, reuse=True)\n",
    "        test_image = np.reshape(input_data, (1, 28, 28, 1))\n",
    "        current_output = output.eval({input_tensor:test_image})\n",
    "    \n",
    "        plt.imshow(input_data, cmap='gray')\n",
    "        print(\"Predicted Image Class: {}\".format(np.argmax(current_output)))  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Predicted Image Class: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enter an integer between 1 and 10000 corresponding to the image in the testing set to be classified\n",
    "integer = 3492\n",
    "\n",
    "\n",
    "data_point_index = integer - 1\n",
    "input_data = x_test[data_point_index]\n",
    "\n",
    "with train_graph.as_default():\n",
    "    classify(input_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function calculates the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy(x_test, y_test):\n",
    "    \n",
    "    input_tensor, output_labels, learning_rate = build_inputs()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        \n",
    "        saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "        \n",
    "        logits, output = build_cnn(input_tensor, reuse=True)\n",
    "\n",
    "        correct = 0\n",
    "\n",
    "        for test_image, test_label in zip(x_test, y_test):\n",
    "            input_data = np.reshape(test_image, (1, 28, 28, 1))\n",
    "            current_output = output.eval({input_tensor:input_data})\n",
    "            class_ = np.argmax(current_output)   \n",
    "            if class_ == test_label:\n",
    "                correct += 1\n",
    "        \n",
    "        accuracy = correct / len(x_test)\n",
    "        print(\"Model accuracy: {}\".format(accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Model accuracy: 0.9759\n"
     ]
    }
   ],
   "source": [
    "# Find the accuracy of this particular model \n",
    "with train_graph.as_default():\n",
    "    model_accuracy(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
